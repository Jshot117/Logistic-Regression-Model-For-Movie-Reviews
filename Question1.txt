z = w * x + b

x = <1, -2, 2, 4, 0>
w = <2, 1, 1, 0, -1>
b = 0

z = (1 * 2) + (-2 * 1) + (2 * 1) + (4 * 0) + (0 * -1) + 0
z = 2 - 2 + 2 = 2

b) Use the sigmoid function to convert the score to the probability: p(y = 1|x).

The sigmoid function is defined as:

sigmoid(z) = 1 / (1 + exp(-z))

sigmoid(2) = 1 / (1 + exp(-2)) ≈ 0.881

So, p(y = 1|x) ≈ 0.881

C)Assume the correct label for this example is pos (y = 1). Compute the cross-entropy loss of

Cross-entropy loss is defined as:

loss = -[y * log(p) + (1 - y) * log(1 - p)]

Since y = 1:

loss = -(1 * log(0.881) + (1 - 1) * log(1 - 0.881))
loss = -log(0.881) ≈ 0.1269

d) Now assume the correct label is neg (y = 0). Compute the cross-entropy loss.

loss = -(0 * log(0.881) + (1 - 0) * log(1 - 0.881))
loss = -log(1 - 0.881) ≈ 2.1269